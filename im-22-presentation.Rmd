---
title: "Cost effectiveness-informed cutpoints for clinical prediction models"
author: "Rex Parsons"
date: "2022-07-15"
output:
  ioslides_presentation:
    widescreen: true
    logo: 'www/aushsi-logo.png'
    css: www/styles.css
runtime: shiny
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(kableExtra)
source("apps/src/utils.R")
```

<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>
<script>
$(document).ready(function() {
  $('slide:not(.backdrop):not(.title-slide)').append('<div class=\"footnotes\">');

  $('footnote').each(function(index) {
    var text  = $(this).html();
    var fnNum = (index+1).toString();
    $(this).html(fnNum.sup());

    var footnote   = fnNum + '. ' + text + '<br/>';
    var oldContent = $(this).parents('slide').children('div.footnotes').html();
    var newContent = oldContent + footnote;
    $(this).parents('slide').children('div.footnotes').html(newContent);
  });
});
</script>

## Outline

- Background to prediction models and cutpoints
- Integrating economic considerations into cutpoint selection
- Simulation study results

## {.fullside}

![](www/intro-boring.png){.center height=600px}


## Computerised decision support systems {.fullside}

![](www/cdss.jpg){.center height=450px}

## Prediction models

>- Do they have it? (diagnostic)
>- Will they get it? (prognostic)


>- Estimates patient risk given some data

## Prediction models

![](www/p-given-data.png){.center height=400px}

## Prediction models


But how do they inform the user?

>- Probability of fall
>- Odds of fall
>- High/medium/low risk categories ðŸš¦
>- Binary categories (intervention vs none)


## Prediction models


But how do they inform the user?


- **Probability of fall**
- **Odds of fall**
- **High/medium/low risk categories**ðŸš¦
- Binary categories (intervention vs none)


## Prediction models


But how do they inform the user?

- Probability of fall
- Odds of fall
- High/medium/low risk categories ðŸš¦
- **Binary categories (intervention vs none)**


## Cutpoints


![](www/sand-1.jpg){.center height=323px}


## Cutpoints

![](www/sand-2.png){.center height=400px}


## Cutpoints


![](www/sand-3.png){.center height=400px}


## {.fullscreen}

```{r, echo=F, message=F, warning=F}
shinyAppDir("apps/prediction-threshold-slider/")
```

## Cutpoint selection methods

All maximise a performance based metric (not outcome) <footnote>Ilker Unal. "Defining an Optimal Cut-Point Value in ROC Analysis: An Alternative Approach." *Computational and Mathematical Methods in Medicine* (2017).</footnote>

- Youden index: $J(c) = Se(c) + Sp(c) - 1$

- Sens-Spec product: $CZ(c) = Se(c) \times Sp(c)$

- Index of Union: $IU(c) = |Se(c) âˆ’ AUC| + |Sp(c) âˆ’ AUC|$

- The Closest to (0, 1) Criteria: $ER(c) = \sqrt{(1 âˆ’ Se(c)^2) + (1 âˆ’ Sp(c)^2)}$

<br>

>- None of these consider anything beyond the model itself (costs, patient outcomes, interventions)


## Objectives


- Integrate information of patient outcomes and treatment costs into cutpoint selection

>- Evaluate how it compares to other cutpoint selection methods using a simulation study


## Evaluation {.build}

Net Monetary Benefit (NMB) is calculated separately for each possible classification.

$$
NMB = COST_{outcome}\times(1-EFFECT_{treatment}) + COST_{treatment}
\\
COST_{outcome} = QALY\times WTP
$$

```{r, echo=F}
data.frame(
  row.names = c("Actually Positive", "Actually Negative"),
  P_Positive = c("TRUE POSITIVE (TP)", "FALSE POSITIVE (FP)"), 
  P_Negative = c("FALSE NEGATIVE (FN)", "TRUE NEGATIVE (TN)")
) %>%
  rename("Predicted Positive" = P_Positive,
         "Predicted Negative" = P_Negative) %>%
  kable() %>%
  kable_styling(font_size=15, full_width=FALSE) %>%
  column_spec(1, bold=T)
```

```{r, echo=F}
data.frame(
  row.names = c("Actually Positive", "Actually Negative"),
  P_Positive = c(
    "$NMB = COST_{outcome}\\times(1-EFFECT_{treatment}) + COST_{treatment}$", 
    "$NMB = COST_{treatment}$"
  ), 
  P_Negative = c(
    "$NMB = COST_{outcome}$",
    "$NMB = 0$"
  )
) %>%
  rename("Predicted Positive" = P_Positive,
         "Predicted Negative" = P_Negative) %>%
  kable() %>%
  kable_styling(font_size=15, full_width=FALSE) %>%
  column_spec(1, bold=T)
```

## Cost-effective cutpoint {.build}

The `{cutpointr}` R package by Christian Thiele includes almost all the cutpoint selection methods methods but also allows you to pass custom metrics that aren't already built in

Applied a custom metric and `{cutpointr}` to maximise NMB: 

- Requires predictions, outcomes, and NMB values assigned to each class (TP, TN, FP, FN)

$n_{TP} \times NMB_{TP} + n_{FP} \times NMB_{FP} + n_{TN} \times NMB_{TN} + n_{FN} \times NMB_{FN}$

<br>

Special thank you to Christian for responding to my twitter DMs and making an update to `{cutpointr}` that made this study easier!

## Simulation study - inpatient falls

- Values for intervention effectiveness, healthcare costs, and patient outcomes literature
  - Patient education intervention
  - Healthcare costs
  - QALYs lost from falls
  - WTP of 28k

>- Used point estimates of these for obtaining the cost-effective-threshold but incorporated uncertainty for evaluation


## Simulation loop{.build}

>1. Sample training data
>2. Fit prediction model
>3. Obtain cutpoints using each method (cost-effective method uses point estimates)
>4. Sample validation data (n=10,000)
>5. Get predicted probabilities for validation data using the fitted model
>6. Get NMB values for each class by sampling costs/effectiveness values from their distributions
>7. Calculate NMB for validation set under each cutpoint (obtained in step 3) and store results

<div style="text-align: right">**Repeat 5,000 times**</div>


## Results - primary analysis

```{r, echo=F, warning=F, message=F, fig.align="center", fig.height=5}
df_result <- readRDS("input/falls_sim_data.rds")
library(bayestestR)
library(ggridges)

names(df_result) <- c(
  "n_sim", "Treat all", "Treat none", "Cost-effective", 
  "The Closest to (0, 1) Criteria", "Youden", "Sens-Spec product", 
  "Index of Union"
)

```

```{r, echo=F, fig.align='center', fig.height=4.8, fig.width=10}
fx_newplot <- function(data) {
  df_long <- 
    data %>%
    pivot_longer(!n_sim) %>%
    group_by(name) %>%
    arrange(value) %>%
    mutate(percentile=row_number()/n()) %>%
    ungroup() %>%
    mutate(in_interval = percentile > 0.025 & percentile < 0.975)
  
  df_agg <- 
    df_long %>%
    group_by(name) %>%
    summarize(m=median(value))
  
  p <-
    df_long %>%
    ggplot(aes(value, fill=in_interval)) +
    geom_histogram(bins=40) + 
    coord_flip() +
    facet_grid(~name) +
    theme_bw() +
    scale_fill_manual(values=c("grey50","grey50", "#ADD8E6")) +
    guides(fill="none") +
    scale_x_continuous(labels=scales::dollar_format()) +
    scale_y_continuous(n.breaks=3) + 
    labs(
      x="Net Monetary Benefit (NMB)\n",
      y="Number of simulations (n)"
    )
  
  my_plot_innards <- ggplot_build(p)
  
  extracted_points <- tibble(
    nmb = my_plot_innards[["data"]][[1]][["x"]],
    count = my_plot_innards[["data"]][[1]][["y"]],
    in_interval = (my_plot_innards[["data"]][[1]][["group"]]) %>% as.factor(),
    method = (my_plot_innards[["data"]][[1]][["PANEL"]] %>% as.character)
  )
  
  heights <- 
    df_agg %>%
    rownames_to_column(var="method_n") %>%
    left_join(extracted_points, by=c("method_n"="method")) %>%
    mutate(diff=abs(m-nmb)) %>%
    group_by(name) %>%
    arrange(diff) %>%
    slice(1)
  
  p + geom_segment(data=heights, aes(x=m, xend=m, y=0, yend=count), size=2, alpha=0.6)
}

select(df_result, -c("Treat all", "Treat none")) %>% fx_newplot()

```

## Results - primary analysis

```{r, echo=F}
df_result %>%
  pivot_longer(!n_sim) %>%
  group_by("Cutpoint method"=name) %>%
  summarize(median_nmb = median(value)) %>%
  arrange(desc(median_nmb)) %>%
  mutate("Median NMB" = scales::dollar_format()(median_nmb)) %>%
  select(-median_nmb) %>%
  kable() %>%
  kable_styling(font_size=15, full_width=FALSE) %>%
  column_spec(1, bold=T)
```


## {.fullscreen}

```{r echo=F, fig.align='center', fig.height=6.5, fig.width=7}
auc_title <- 
  cowplot::ggdraw() + 
  cowplot::draw_label("Benefit is greater for less discriminant models", fontface='bold')
cowplot::plot_grid(auc_title, readRDS("input/auc_plot.rds"), rel_heights=c(0.05,1), ncol=1)
```


## {.fullscreen}

```{r echo=F, fig.align='center', fig.height=6.5, fig.width=7}
event_rate_title <- 
  cowplot::ggdraw() + 
  cowplot::draw_label("Benefit is greater for rarer events", fontface='bold')
cowplot::plot_grid(event_rate_title, readRDS("input/event_rate_plot.rds"), rel_heights=c(0.05,1), ncol=1)
```


## Conclusions

- Considering costs when selecting a model cutpoint may facilitate value-based care

- The best approach may be to treat-all or treat-none


## shiny app

```{r, echo=F, warning=F, message=F}
shinyAppDir("apps/cost-effective-cpms-app/")
```

## Thank you

Team

  - Robin Blythe
  - Susanna Cramb
  - Steven McPhail

![](www/twitter-logo.png){width=30 align="left"} @Rex_Parsons8  

![](www/github-logo.png){width=30px align="left"} @RWParsons

![](www/website-icon.png){width=30px align="left"} http://rwparsons.github.io/

shiny app: [aushsi.shinyapps.io/cost-effective-cpms-app/](https://aushsi.shinyapps.io/cost-effective-cpms-app/)

slides: [github.com/RWParsons/im-22-presentation](https://github.com/RWParsons/im-22-presentation)
